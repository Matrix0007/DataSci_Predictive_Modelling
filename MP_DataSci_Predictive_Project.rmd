---
title: "Data Science Project - Predictive Model"
author: "by MP"
date: "Dec 2019"
output:
  pdf_document:
    toc: yes
    toc_depth: '4'
  html_document:
    df_print: paged
    number_sections: yes
    theme: readable
    toc: yes
    toc_depth: 4
  word_document: default
---
```{r setup, include=FALSE}
# knitr set
knitr::opts_chunk$set(eval = FALSE)
```

# Introduction 
  
  
  
In this project we look at the Boston House Price dataset to work through a regression predictive model for predicting the housing prices in Boston. We first analyse the data and then try to predict Boston house value with a model. In order to refine the accuracy of the model, we will transform the data, use algorithms to tune data and apply ensemble methods of bagging and boosting.  
  
  
  


## Project Overview  
  
  
Below is a quick overview of the project.  
  


```{r, project_overview, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE, fig.width=10, fig.height=10, fig.align = "center"}
# Project Overview Visual
knitr::include_graphics("https://github.com/Matrix0007/datasets/blob/master/Project_Overview.png?raw=true")
```

# Load data packages and data

We need to load the pakages as listed below;  

```{r load_packages, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
#load packages if required
if(!require(mlbench)) install.packages("mlbench", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(lattice)) install.packages("lattice", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(Cubist)) install.packages("Cubist", repos = "http://cran.us.r-project.org")
if(!require(jpeg)) install.packages("jpeg", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")

library(mlbench)
library(caret)
library(corrplot)
library(tibble)
library(Cubist)
library(jpeg)
library(readr)
library(tidyverse)
library(knitr)
```

Once the packages are loaded, we download the data from the web by using the below code;  

```{r load_data_from_URL, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
#load data from url
dta <- url("http://course1.winona.edu/bdeppa/Stat%20425/Data/Boston_Housing.csv")
TBdta <- as.data.frame(read.csv(dta, check.names = FALSE))
```

# Validation Dataset

It is a good idea to use a validation hold out set. This is a sample of the data that we hold
back from our analysis and modeling. We use it right at the end of our project to confirm the
accuracy of our final model. It is a smoke test that we can use to see if we messed up and to
give us confidence on our estimates of accuracy on unseen data.
```{r validation, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Split out validation dataset
# create a list of 80% of the rows in the original dataset we can use for training
set.seed(7)
validateIndex <- caret::createDataPartition(TBdta$MEDV, p=0.80, list=FALSE)
# select 20% of the data for validation
validateSet <- TBdta[-validateIndex,]
# use the remaining 80% of data to training and testing the models
DTset <- TBdta[validateIndex,]

```
# Analyse Data

The objective of this step in the process is to better understand the problem.  

## Data Observations  

Each record in the database describes a Boston suburb or town. The data was drawn from the
Boston Standard Metropolitan Statistical Area (SMSA) in 1970. The attributes are defined as
Follows;

1. CRIM: per capita crime rate by town  
2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.  
3. INDUS: proportion of non-retail business acres per town  
4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)  
5. NOX: nitric oxides concentration (parts per 10 million)  
6. RM: average number of rooms per dwelling  
7. AGE: proportion of owner-occupied units built prior to 1940  
8. DIS: weighted distances to  

### Dimensions and the class

```{r review_data, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

# check dimensions of DTset
dim(DTset)

# review attribute types
sapply(DTset, class)
```
The above results show that;  
•	There are 407 instances and 14 attributes  
•	We also note the attributes as "numeric" and "integer"   

### Snapshot of data

To understand the data better we review the first 10 rows of data.  
```{r review_first_10_rows, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

# data first 10 rows
head(DTset, n=10)
```

We note that the scale range for the attributes are very broad due to the varied unit. Transformation of these can assist us.

### Distribution of the attributes

Below is a summary of attributes distribution.
```{r summary_attributes, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

# summary of the attributes
summary(DTset)
```

### Correlation


Correlation is a statistical technique that can show whether and how strongly pairs of variables are related (www.surveysystem.com).  
Below is a reveiw of the correlation between all of the numeric attributes.
```{r Correlation, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

# Correlation of the attributes
cor(DTset[,1:13])
```
The above results show that a numbers of the atrributes as listed below have a high correlation.  

•	NOX & INDUS have a correlation of 0.77  
•	DIS & INDUS have a correlation of 0.71  
•	AGE & NOX have a correlation of 0.73  
•	DIS & NOX have a correlation of 0.77  
•	TAX & RAD have a correlation of 0.92  
  
This high correlation is known as Collinearity. Collinearity in statistics, correlation between predictor variables (or independent variables), such that they express a linear relationship in a regression model. When predictor variables in the same regression model are correlated, they cannot independently predict the value of the dependent variable. In other words, they explain some of the same variance in the dependent variable, which in turn reduces their statistical significance (Felicity, 2019).  
  
It would be better if we were to remove this collinearity to improve regression algorithms results.  

## Visualise the data  

Data visualization is the graphic representation of data. It involves producing images that communicate relationships among the represented data to viewers of the images. This communication is achieved through the use of a systematic mapping between graphic marks and data values in the creation of the visualization (Wikipedia, 2019)  

We can now review and visualise the data.  

### Box/Whisker Plots 

```{r boxplot, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE, fig.width=10, fig.height=7, fig.align = "center"}
# Boxplot visual of the attributes
par(mfrow=c(2,7))
for(i in 1:13){
  boxplot(DTset[,i], main=names(DTset)[i], col="deepskyblue", fg="mediumorchid", col.axis="darkgreen")
}
```
  
The above results represents large varainces in the distributions. The data looks like outliers, outside the whisker of the plots.  

### Histograms 

```{r histogram, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE, fig.width=10, fig.height=7, fig.align = "center"}
# histograms each attribute
par(mfrow=c(2,7))
for(i in 1:13) {
hist(DTset[,i], main=names(DTset)[i], col="deepskyblue", fg="mediumorchid", col.axis="darkgreen")
}
```
  
The above results show;  
•	B, Age, ZN and CRIM has exponential distribution ()  
•	TAX and RAD has bimodal distribution  
  

### Density Plots  

```{r density, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE, fig.width=10, fig.height=7, fig.align = "center" }
# density plot for each attribute
par(mfrow=c(2,7))
for(i in 1:13) {
plot(density(DTset[,i]), main=names(DTset)[i], col="blue", fg="mediumorchid", col.axis="darkgreen")
}
```
  
The above density plots suggest that LSTAT, RM and NOX are like skewed Normal/Gaussian distributions.  

### Scatter Plots  

```{r scatter_plot, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE, fig.width=10, fig.height=10, fig.align = "center"}
# Scatter Plots
pairs(DTset[,1:13], col="blue", fg="mediumorchid", col.axis="darkgreen")
```
    
We note the structure of the correlated attributes as predictive relationships.  

### Correlated Plot
  
    
      
      
```{r correlated_plot, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE, fig.width=7, fig.height=7, fig.align = "center" }
# correlated plot
correlated<- cor(DTset[,1:13])
corrplot(correlated, type = "upper", order = "hclust", col = c("purple", "blue"), bg = "darkgrey")
```   


  
As we can see from this plot the large blue circles are positively correlated where-as the larger purple circles are negatively correlated. We need to think of removing these to improve the accuracy of predictive models.

## Observation and next steps
  
We have reviewed the data and note the detail around the structure of the dataset. In order to improve modelling accuracy we need to modify and transform our data e.g.:-  
  
•	Reduce the impact of the differing scales by normalising the dataset  
•	Reduce the effect of differing distributions via dataset standardisation  
•	Apply Box Cox transformation to normalise and flatten out some of the distributions to improve accuracy   
•	Remove highly correlated attributes and feature selection  


    
    
# Evaluate Baseline Algorithm  

We need to test out a few algorithms to find out which one works well. 

Firstly we use 10 fold cross validation with 3 repeats. Our data is a good size set to use for testing. We will use RMSE go get an understanding how wrong our predictions are and R2 will be used to understand how well the model has fit the data.   
  
  
```{r 10_fold, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# 10-fold cross validation
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
```   

Next we create a baseline of performance and check a number of different algorithms.  

Linear Algorithms we use will be;  
•	LR: Linear A Linear Regression  
•	GLM: Generalized Linear Regression  
•	GLMNT: Penalized Linear Regression  
  
Nonlinear Algorithms we use will be;  
•	CART: Classification and Regression Trees  
•	SVM: Support Vector Machines  
•	KNN: k-Nearest Neighbours  
  
```{r baseline, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}  
# LM
set.seed(7)
data.lm <- train(MEDV~., data=DTset, method="lm", metric=metric, preProc=c("center",
"scale"), trControl=trainControl)
# GLM
set.seed(7)
data.glm <- train(MEDV~., data=DTset, method="glm", metric=metric, preProc=c("center",
"scale"), trControl=trainControl)
# GLMNET
set.seed(7)
data.glmnet <- train(MEDV~., data=DTset, method="glmnet", metric=metric, preProc=c("center", "scale"), trControl=trainControl)
# SVM
set.seed(7)
data.svm <- train(MEDV~., data=DTset, method="svmRadial", metric=metric,
preProc=c("center", "scale"), trControl=trainControl)
# CART
set.seed(7)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
data.cart <- train(MEDV~., data=DTset, method="rpart", metric=metric, tuneGrid=grid,
preProc=c("center", "scale"), trControl=trainControl)
# KNN
set.seed(7)
data.knn <- train(MEDV~., data=DTset, method="knn", metric=metric, preProc=c("center",
"scale"), trControl=trainControl)
```
  

We now compare our algorithms and summarise the results.  
```{r compare, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}  
# Review our algorithms
baseline.results <- resamples(list(LM=data.lm, GLM=data.glm, GLMNET=data.glmnet, SVM=data.svm,
CART=data.cart, KNN=data.knn))
summary(baseline.results)
```
  
The results suggest SVM to have the lowest RMSE followed by CART and KNN. We also note that SVM and other nonlinear algorithms have the best fit for the data in their R2 measures.  

Dotplot Visual  
```{r baseline.results_dotplot, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE,  fig.width=10, fig.height=7, fig.align = "center"} 
# Visualise via dotplot
dotplot(baseline.results)
```
  
# Feature Selection Algorithms Review  
  
  

There is a belief that correlated attributes reduce the accuracy of the linear algorithms in the baseline spot-check in the last step. Here we will remove the highly correlated
attributes and see what effect this that has on the evaluation metrics.    

```{r exclude_correlated, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE, fig.width=10, fig.height=7, fig.align = "center"} 
# Exclude correlated attributes

# Look for correlated attributes
set.seed(7)
cutoff <- 0.70
correlations <- cor(DTset[,1:13])
CorrelatedData <- findCorrelation(correlations, cutoff=cutoff)
for(value in CorrelatedData)
  {
print(names(DTset)[value])
}
# create data exluding correlated features
DTsetFeatures <- DTset[,-CorrelatedData]
dim(DTsetFeatures)
```
The above has resulted in excluding these four attritibutes;  
1. INDUS  
2. NOX  
3. TAX  
4. DIS  

Next we re-run our six algorithms from the baseline.  


```{r feature, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Algorithms with 10 fold cross validation
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
# lm
set.seed(7)
data.lm <- train(MEDV~., data=DTsetFeatures, method="lm", metric=metric,
preProc=c("center", "scale"), trControl=trainControl)
# GLM
set.seed(7)
data.glm <- train(MEDV~., data=DTsetFeatures, method="glm", metric=metric,
preProc=c("center", "scale"), trControl=trainControl)
# GLMNET
set.seed(7)
data.glmnet <- train(MEDV~., data=DTsetFeatures, method="glmnet", metric=metric,
preProc=c("center", "scale"), trControl=trainControl)
# SVM
set.seed(7)
data.svm <- train(MEDV~., data=DTsetFeatures, method="svmRadial", metric=metric,
preProc=c("center", "scale"), trControl=trainControl)
# CART
set.seed(7)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
data.cart <- train(MEDV~., data=DTsetFeatures, method="rpart", metric=metric,
tuneGrid=grid, preProc=c("center", "scale"), trControl=trainControl)
# KNN
set.seed(7)
data.knn <- train(MEDV~., data=DTsetFeatures, method="knn", metric=metric,
preProc=c("center", "scale"), trControl=trainControl)
# Compare algorithms
feature.results <- resamples(list(LM=data.lm, GLM=data.glm, GLMNET=data.glmnet, SVM=data.svm,
CART=data.cart, KNN=data.knn))
summary(feature.results)
```
  

We note that removal of the correlated attributes are making the RMSE worse for both the linear  and non linear algorithms. Removing of correlated attributes improves the accuracy.  

```{r feature.results_dotplot, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE, fig.width=10, fig.height=7,fig.align = "center"} 
# Visualise via dotplot
dotplot(feature.results)
```
  
# Box-Cox Transform Algorithm Review  
  
Our data attributes is skewed with exponential distribution.  We will use Box-Cox transformation to rescale the data and review what impact it has on our six algorithms.  

```{r box-cox, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}

# Run algorithms using 10-fold cross-validation
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
# lm
set.seed(7)
data.lm <- train(MEDV~., data=DTset, method="lm", metric=metric, preProc=c("center",
"scale", "BoxCox"), trControl=trainControl)
# GLM
set.seed(7)
data.glm <- train(MEDV~., data=DTset, method="glm", metric=metric, preProc=c("center",
"scale", "BoxCox"), trControl=trainControl)
# GLMNET
set.seed(7)
data.glmnet <- train(MEDV~., data=DTset, method="glmnet", metric=metric,
preProc=c("center", "scale", "BoxCox"), trControl=trainControl)
# SVM
set.seed(7)
data.svm <- train(MEDV~., data=DTset, method="svmRadial", metric=metric,
preProc=c("center", "scale", "BoxCox"), trControl=trainControl)
# CART
set.seed(7)
grid <- expand.grid(.cp=c(0, 0.05, 0.1))
data.cart <- train(MEDV~., data=DTset, method="rpart", metric=metric, tuneGrid=grid,
preProc=c("center", "scale", "BoxCox"), trControl=trainControl)
# KNN
set.seed(7)
data.knn <- train(MEDV~., data=DTset, method="knn", metric=metric, preProc=c("center",
"scale", "BoxCox"), trControl=trainControl)
# Compare algorithms
box.cox.results <- resamples(list(LM=data.lm, GLM=data.glm, GLMNET=data.glmnet, SVM=data.svm,
CART=data.cart, KNN=data.knn))
summary(box.cox.results)
```

```{r box-cox.results_dotplot, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE, fig.width=10, fig.height=7,fig.align = "center"} 
# Visualise via dotplot
dotplot(box.cox.results)
```
  
Now we note decreased RMSE and increased R2 on all except the CART algorithms.  
  
# Tuning to improve accuracy  
  
To improve the accuracy of the algorithms we can tune their parameters. Here, we will tune SVM parameters with RBF (Radial Basis Function).  
  
Let us see the estimated accuracy of our model.  

```{r print_data.svm, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE} 
# Print DataSVM fiited model
print(data.svm)
```

Next steps;  
•	We build a grid search around C value of 1  
•	We may note a reduction in the RMSE with increase in C. We shall try C values of 1 to 10  
•	CARET package allows the sigma parameter to be tuned for smoothing. Ideally values initiating 0.1 are good.   


```{r tunning_SVM_C, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE, fig.width=10, fig.height=7, fig.align = "center"} 
# Tunning SVM and C
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
set.seed(7)
grid <- expand.grid(.sigma=c(0.025, 0.05, 0.1, 0.15), .C=seq(1, 10, by=1))
data.svm <- train(MEDV~., data=DTset, method="svmRadial", metric=metric, tuneGrid=grid,
preProc=c("BoxCox"), trControl=trainControl)
print(data.svm)
```
  
In the results we note that sigma values flatten out with larger C cost. The final values used for the model were sigma = 0.1 and C = 9.  
```{r plot_data.svm, echo = TRUE, message = FALSE, warning = FALSE, fig.width=10, fig.height=7,fig.align = "center", eval = TRUE}
# Plot tunning results
plot(data.svm)
```
  
# Ensemble methods Alogrithms  
  
In this section we will attempt to further reduce RMSE by applying ensemble methods like boosting and bagging techniques for decision trees.

Here we apply;  
•	RF; Random Forest, bagging  
•	CUBIST boosting   
•	GBM; Gradient Boosting Machines  
  
```{r Ensembles, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Ensembles
seed <- 7
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
# RF
set.seed(seed)
ensemble.rf <- train(MEDV~., data=DTset, method="rf", metric=metric, preProc=c("BoxCox"),
trControl=trainControl)
# CUBIST
set.seed(seed)
ensemble.cubist <- train(MEDV~., data=DTset, method="cubist", metric=metric,
preProc=c("BoxCox"), trControl=trainControl)
# GBM
set.seed(seed)
ensemble.gbm <- train(MEDV~., data=DTset, method="gbm", metric=metric, preProc=c("BoxCox"),
trControl=trainControl, verbose=FALSE)

# Evaluate resultsalgorithms
ensembles.output <- resamples(list(RF=ensemble.rf, CUBIST=ensemble.cubist, GBM=ensemble.gbm))
summary(ensembles.output)
```
  
  
The results show us that Cubist resulted in the lowest RMSE than all the others.  

```{r dotplot_ensembles, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE, fig.width=10, fig.height=7,fig.align = "center"}
# Plot tunning results
dotplot(ensembles.output)
```
  
Next, we into tuning Cubist further by using two of the Caret parameters;  
•	Committees: The tree-based Cubist model can be easily used to develop an ensemble classifier with a scheme called “committees”. The concept of “committees” is similar to the one of “boosting” by developing a series of trees sequentially with adjusted weights. However, the final prediction is the simple average of predictions from all “committee” members, an idea more close to “bagging” (Statcompute, 2015)  
•	Neighbors; numbers of instances used for prediction  
  
Detailed review of Cubist  

```{r review_Cubist, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Detailed review of Cubist
print(ensemble.cubist)
```
  
As we note from the above summary, the final values used for the model were committees = 20 and neighbors = 5.  

Next, we apply grid search to tune our values using all committees, 15 to 25. We also review for a neighbors over and under 5.  

```{r tunning_Cubist, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Tunning Cubist
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "RMSE"
set.seed(7)
grid <- expand.grid(.committees=seq(15, 25, by=1), .neighbors=c(3, 5, 7))
cubist.tuned <- train(MEDV~., data=DTset, method="cubist", metric=metric,
preProc=c("BoxCox"), tuneGrid=grid, trControl=trainControl)
print(cubist.tuned)
```
  
Our RMSE improved to 3.027991 with committees = 25 and neighbors = 3.  

```{r cubist.tuned, echo = TRUE, message = FALSE, warning = FALSE, fig.width=10, fig.height=7,fig.align = "center", eval = TRUE}
# Plot tunned Cubist
plot(cubist.tuned)
```
  
  
# Optimal Algorithm 

From our modelling and its outcomes, Cubist seems to the accurate one.  
  
Next, we generate an independent Cubist model from the above parameters with the complete dataset using Box-Cox transformation.  

```{r Cubist.model, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Data preparation
set.seed(7)
x <- DTset[,1:13]
y <- DTset[,14]
prep.Parms <- preProcess(x, method=c("BoxCox"))
transformX <- predict(prep.Parms, x)
# training Cubist Model
cubist.model <- cubist(x=transformX, y=y, committees=18)
```
  
  
The view the full summary we can insert this code into our RMD chunk;  
summary(cubist.model)  

```{r modify.model, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Transforming validation dataset
set.seed(7)
validation.X <- validateSet[,1:13]
transform.validation.X <- predict(prep.Parms, validation.X)
validation.Y <- validateSet[,14]
# Predictions based on Cubist model with the validation data
predict.output <- predict(cubist.model, newdata=transform.validation.X, neighbors=3)
# Final RMSE
Final.rmse <- RMSE(predict.output, validation.Y)
r_2 <- R2(predict.output, validation.Y)
print(Final.rmse)
```
  
We note that the RMSE output from our Cubist model is 3.237403 which is a little low but quite similar to the expected RMSE of 3.027991.  

# Conclusion  
  
We have applied a regression predictive machine learning model in this project. We used the Boston housing data where we first analysed the data where we found skewed distributions and correlations. Next, we evaluated a number of algorithms, removed correlated attributes, transformed & tuned the data. Finally we applied ensemble methods of bagging and boosting on the Cubist.  

  
  
# References  

## Articles

•	Irizzary,R., “Introduction to Data Science,github page,https://rafalab.github.io/dsbook/”, (2018)  
•	Felicity Boyd Enders, “Collinearity, Statistics”- https://www.britannica.com/topic/collinearity-statistics    
•	Data visualization, Wikipedia - https://en.wikipedia.org/wiki/Data_visualization, 2019  
•	Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) The New S Language. Wadsworth & Brooks/Cole  
•	Vishal R, “Feature selection — Correlation and P-value”, 2018  
  
  
## Web Links  

•	https://holtzy.github.io/Pimp-my-rmd/  
•	https://www.surveysystem.com/correlation.htm  
•	http://www.stat.tamu.edu/~hart/652/collinear.pdf  
•	https://www.britannica.com/topic/collinearity-statistics  
•	https://www.statisticshowto.datasciencecentral.com/box-cox-transformation/  
•	https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf  
•	https://statcompute.wordpress.com/2015/03/21/ensemble-learning-with-cubist-model/  
•	https://static1.squarespace.com/static/51156277e4b0b8b2ffe11c00/t/56e3056a3c44d8779a61988a/1457718645593/cubist_BRUG.pdf  
